# -*- coding: utf-8 -*-
"""Gazprom.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nQpd1QpWSCC9_LrFzW10TauRz3v08zbo

# Оглавление
[Импорт библиотек](#Import) \
[Знакомство с данными](#Glance) \
[Пропроцессинг данных](#Preprocessing) \
[Создание текстовых фичей](#Text) \
[Разведочный анализ данных](#EDA) \
[Разделение на трейн и тест](#Split) \
[Завершение препроцессинга](#Preprocessing_end) \
[Отбор фичей](#Selection) \
[Выбор модели](#Model) \
[Гиперпараметры](#hyperparams) \
[Оценка качества](#quality) \

# Import
Импорт библиотек
"""

pip install catboost

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import chardet # для определения энкодинга
import nltk # пакет работы с текстами
from nltk.sentiment import SentimentIntensityAnalyzer

from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.tree import DecisionTreeRegressor, plot_tree

# метрики
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, explained_variance_score

# отбор фичей
from sklearn.feature_selection import VarianceThreshold
from sklearn.feature_selection import mutual_info_regression
from sklearn.linear_model import Lasso
from sklearn.preprocessing import StandardScaler
from sklearn.feature_selection import RFE
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import ElasticNet
from catboost import CatBoostRegressor
import lightgbm as lgb
from sklearn.linear_model import ElasticNet

# определим энкодинг, тк при прямом считывании в лоб вылетают ошибки
with open('realty_data.csv', 'rb') as rawdata:
    result = chardet.detect(rawdata.read(100000))
print(result['encoding'])

# считываем датасет
# При попытке импорта файла возникают ошибки. Данная конфигурация позволяет импортирвоть файл
# Была убрана только 1 строка Оригинальный размер - 98823, импортированный 98822
df = pd.read_csv('realty_data.csv', encoding='utf-8', on_bad_lines='skip', sep=',',  engine='python')

df.shape

"""# Glance
Первичное знакомство с данными

Описание столбцов:

product_name - наименование объекта недвижимости- \
period - дата \
postcode - почтовый индекс \
address_name - адрес объекта \
lat - координаты (широта) \
lon - координаты (долгота) \
object_type - тип объекта \
total_square - общая площадь \
rooms - количество комнат \
floor - этаж \
city - город \
settlement - населенный пункт \
district - район \
area - микрорайон \
description - текст объявления \
source - источник \
"""

df.shape

df.head()

df['price'].plot(kind='hist', bins=20, title='Стоимость недвижимости')
plt.xlabel('Количество')
plt.ylabel('Стоимость')
plt.gca().spines[['top', 'right',]].set_visible(False)

# какие города в выборке
df.city.unique()

# какие источники в выборке
df.source.unique()

"""# Preprocessing
Пропроцессинг данных \
Обычно я делаю EDA. В данном случае нужно выделить фичи из текста.
По части внесения доп данных из широты и долготы - сложно найти сервис, который бы внес нечто полезное в данные и при этом был бесплатный и был был готов обработать около 100000 записей. Бесплатные сервисы не вносят полезной информации. \
Что рассматривал: \
Яндекс.Карты \
гугл карты \
2гис \
Azure Maps \
MapBox \
DaData \
Бесплатные сервисы могут предоставить полезного только разные государственные коды, такие как ФИАС, КЛАДР, ОКАТО, ОКТМО, ИФНС. При этом, сами по себе они несут маленькую полезность, нужно добывать доплнительные коннекторы, чтобы получить полезные данные, такие как проходимость, рыночная стоимость расстояние от метро, от кольцевой или тип дома. \
Их можно получить только в платных версиях

"""

# выделю размер квартиры через регулярное выражение
df['size'] = df['product_name'].str.extract(r'(\d+,\d+|\d+)\s*м²')[0].str.replace(',', '.').astype(float)

# есть ли ошибки
df['size'].isnull().sum() # ошибок нет

df.head()

# эту переменную можно удалить
df.period.value_counts()

del df['period']

# уникальных значений по комнатности квартиры ограниченное количество
df['product_name'].str.split(',').str[0].unique()

df['product_name'].isna().sum()

# количество комнат
df['rooms'] = df['product_name'].str.extract(r'(\d+)-комнатная')[0].astype(float)
print(df['rooms'].isna().sum()) # проверю что это именно студии

# попалась квартира без количества комнат
df.loc[pd.isna(df['rooms']), ['rooms', 'product_name']].head()

# выделю только студии
df['flag_studio'] = df['product_name'].str.contains(r'\bСтудия\b')

# сколько будет записей без квадртаного метра и студии
df.loc[pd.isna(df['rooms']) & df['flag_studio'] == True, ['rooms', 'product_name']].shape
# значит количество настоящих пустых значений - 7061 - 6409 = 652

# Пусть все студии будут считаться, как однокомнатные квартиры
df.loc[pd.isna(df['rooms']) & df['flag_studio'] == True, ['rooms']] = 1

# оставшиеся заполню средним по метражу
avg_size_grouping = df[df['rooms'].notna()].groupby('rooms')['size'].mean()
def find_nearest_rooms(size):
    return avg_size_grouping.index[np.abs(avg_size_grouping - size).argmin()]

df.loc[pd.isna(df['rooms']), 'rooms'] = df.loc[pd.isna(df['rooms']), 'size'].apply(find_nearest_rooms)

# проверим количество незаполненных значений
df['rooms'].isna().sum() # все ок

# удалю промежуточные и ненужные колонки
df = df.drop(columns=['product_name', 'flag_studio', 'lat', 'lon'], axis=1)

df.head()

"""Почтовые индексы России состоят исключительно из шести цифр XXXYYY, где XXX – код города, а YYY — номер почтового отделения. В некоторых особенно крупных городах введено несколько кодов. Например, это правило действует для индекса Москвы. Далее к коду добавляется номер почтового отделения.
Вывод - почтовый индекс не принесет новой полезной инфорамции. Единственное, что могу учесть - это факт наличия пустоты.
"""

df['flag_postcode'] = df['postcode'].isna()
df['flag_postcode'].fillna(False, inplace=True)
df = df.drop(columns=['postcode'], axis=1)

df.isna().sum() # в общем проусков мало, больше всех в поселении, потому что там деревни и зоне

# из адреса можно выделить только тип - улица, проспект, другое, что может говорить об оживленности в районе
# потенциально уровень коммерциализации, зашумленности, развития. Хотя в большинстве адресов не указан тип
df['flag_bulvar'] = df['address_name'].str.contains(r'\b[Б|б]ульвар\b').fillna(False)
df['flag_prospect'] = df['address_name'].str.contains(r'\b[П|п]роспект\b').fillna(False)
df['flag_ulitsa'] = df['address_name'].str.contains(r'\b[У|у]лица\b').fillna(False)
df['flag_shosse'] = df['address_name'].str.contains(r'\b[Ш|ш]оссе\b').fillna(False)
df['flag_proezd'] = df['address_name'].str.contains(r'\b[П|п]роезд\b').fillna(False)
df['flag_ZK'] = df['address_name'].str.contains(r'\bЖК\b').fillna(False)
df['flag_drugoe'] = (~df['flag_bulvar'] & ~df['flag_prospect'] & ~df['flag_ulitsa'] & ~df['flag_shosse'] & ~df['flag_proezd'] & ~df['flag_ZK'])

df = df.drop(columns=['address_name'], axis=1)

df.head()

print(df['object_type'].unique())
df = df.drop(columns=['object_type'], axis=1) # только 1 тип данных. Полезной информации не несет

df.isna().sum()

df.head()

# Предлагаю сделать группировку по локации, используя субьективные знания о мире :)
def get_location_category(df: pd.DataFrame):
  if df['city'] == 'Москва':
    return 1
  elif df['city'] in ['Балашиха', 'Люберцы', 'Красногорск', 'Химки', 'Мытищи',
                      'Котельники', 'Одинцово','Дзержинский', 'Реутов', 'Московский','Долгопрудный']:
    return 2
  elif df['city'] in ['Щёлково', 'Королёв', 'Щербинка', 'Подольск', 'Видное', 'Лыткарино', 'Пушкино',
                      'Ивантеевка', 'Лобня']:
    return 3
  elif pd.notna(df['settlement']):
    return 4
  else:
    return 5

df['location_category'] = df[['settlement', 'city']].apply(get_location_category, axis=1)

# теперь обращу внимание на тип поселения settlement
# тут значения либо nan либо имя поселка. Имя мне ни о чем не говорит. Предлагаю их объединить районы Москвы
# и названия поселков, так что получится получить некоторые средние по району или поселку, потом можно
# будет дальше вычислить отношения между ними
df['district'].fillna(df['settlement'], inplace=True)

# удаление колонки, которая не пригодится в будущем
df = df.drop(columns=['settlement'], axis=1)

# Введем понятие квадратного метра
df['square_meter'] = df['total_square'] / df['rooms']

# И стоимости квадратного метра
# Решил убрать этот показатель, потому что это дает слишком большую подсказку модели. Цена - это тарегет
# df['price_square_meter'] = df['price'] / df['total_square']

# size колонка получилась лишней, есть total_square
df.drop(columns=['size'], inplace=True)

# по колонке 'area' - слишком много пропусков и слишком трудно их будет сметчить, удаляю
df.drop(columns=['area'], inplace=True)

# также преобразуем таргет, сделав распределение более нормальным, что является необходимым для линейной регрессии
df['log_price'] = np.log(df['price'])

# dummy
df['source'] = df['source'].replace(['ЦИАН', 'Домклик', 'Яндекс.Недвижимость', 'Новострой-М'], ['zian', 'domclick', 'yandex', 'novostroi'])
dummy_variables = pd.get_dummies(df['source'], prefix='source')

# Concatenate dummy variables with original DataFrame
df = pd.concat([df, dummy_variables], axis=1)

# Optionally, drop the original 'source' column
df = df.drop('source', axis=1)

"""# Text
Обработка и создание текстовых фичей
"""

# количество слов вообще
df['description_count_words'] = df['description'].apply(lambda x: len(str(x).split()) if pd.notna(x) else 0)

# количество слов в верхнем регистре
df['description_count_upper'] = df['description'].apply(lambda x: sum(1 for word in str(x).split() if word.isupper()) if pd.notna(x) else 0)

# количество цифр
df['description_count_digits'] = df['description'].apply(lambda x: sum(1 for char in str(x) if char.isdigit()) if pd.notna(x) else 0)

# количество знаков !
df['description_count_exclamation'] = df['description'].apply(lambda x: str(x).count('!') if pd.notna(x) else 0)

# количество знаков ?
df['description_count_question'] = df['description'].apply(lambda x: str(x).count('?') if pd.notna(x) else 0)

# количество знаков ,
df['description_count_comma'] = df['description'].apply(lambda x: str(x).count(',') if pd.notna(x) else 0)

# количество символов в тексте
df['description_count_symbols'] = df['description'].apply(lambda x: len(str(x)) if pd.notna(x) else 0)

# количество слов в малом регистре
df['description_count_lower'] = df['description'].apply(lambda x: sum(1 for word in str(x).split() if word.islower()) if pd.notna(x) else 0)

# Отношение количества символов к количеству слов
df['description_ratio_symbols_words'] = df['description_count_symbols'] / df['description_count_words']

# отношение количества слов в верхнем регистре к колчеству слов в нижнем регистре
df['description_ratio_upper_lower'] = df['description_count_upper'] / df['description_count_lower']

# В этом тексте есть много работы над поиском именованных сущеностей, различные виды анализа.
# Я использую пакет nltk
nltk.download('vader_lexicon')

def get_sentiment(text):
    if isinstance(text, float):
        text = str(text)

    sia = SentimentIntensityAnalyzer()
    sentiment_scores = sia.polarity_scores(text)

    if sentiment_scores['compound'] >= 0.05:
        return 1
    elif sentiment_scores['compound'] <= -0.05:
        return -1
    else:
        return 0

df['sentiment'] = df['description'].apply(get_sentiment)

df['sentiment'].notna().sum() == df.shape[0] # все ок

# больше колонка description не пригодится
df.drop(columns=['description'], inplace=True)

"""# EDA"""

# визуадихация количества клиентов по источнику
df_tmp = pd.read_csv('realty_data.csv', encoding='utf-8', on_bad_lines='skip', sep=',',  engine='python')
df_tmp.groupby('source').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))
plt.gca().spines[['top', 'right',]].set_visible(False)

df['log_price'].plot(kind='line', figsize=(8, 4), title='Логарифмированная цена')
plt.gca().spines[['top', 'right']].set_visible(False)

df['log_price'].plot(kind='hist', bins=100, title='Логарифмированная цена')
plt.gca().spines[['top', 'right',]].set_visible(False)

# распределение очень сдвинуто в сторону хвоста, даже проверять тестом не нужно. Это не нормальное распределение
df['price'].plot(kind='hist', bins=100, title='Обычная цена')
plt.gca().spines[['top', 'right',]].set_visible(False)

df.plot(kind='scatter', x='price', y='total_square', s=32, alpha=.8)
plt.gca().spines[['top', 'right',]].set_visible(False)

df.columns

# для более быстрой отрисовки, оставлю только 20000 записей
sample_size = 20000  # или другое подходящее число
df_sample = df.sample(n=sample_size, random_state=42)
sns.pairplot(df_sample,
             vars=['price_square_meter', 'rooms', 'floor', 'log_price','total_square', 'location_category', 'square_meter', 'description_count_words', 'description_count_symbols', 'description_ratio_symbols_words', 'sentiment'],  # выбор конкретных колонок
             kind='scatter',  # тип графика (можно использовать 'reg' для добавления линии регрессии)
             diag_kind='kde',  # тип графика на диагонали ('hist' для гистограммы, 'kde' для графика плотности)
             plot_kws={'alpha': 0.6},  # настройки для scatter plot
             diag_kws={'shade': True})  # настройки для графиков на диагонали

plt.show()

"""# Split
Разделение на трейн и тест
"""

X_train, X_test, y_train, y_test = train_test_split(df.drop(columns=['log_price', 'price']), df['log_price'], test_size=0.2, random_state=42)

"""# Preprocessing_end
Завершение препроцессинга
"""

# добавлю медиану с группировкой этажности, количеству комнат, общей площади, цене по каждому району Москвы или поселению (где нет района)
# медиана среднего размера комнаты. Оберну в функции, потому что в трейне и тесте будут разные значения
def get_medians_district(df: pd.DataFrame):
  # df['median_district_price'] = df.groupby('district')['price'].transform('median')
  df['median_distict_total_square'] = df.groupby('district')['total_square'].transform('median')
  df['median_distict_rooms'] = df.groupby('district')['rooms'].transform('median')
  df['median_distict_floor'] = df.groupby('district')['floor'].transform('median')
  df['median_district_square_meter'] = df.groupby('district')['square_meter'].transform('median')

# сделаю то же самое по группировке по городу
def get_medians_city(df: pd.DataFrame):
  # df['median_city_price'] = df.groupby('city')['price'].transform('median') # не беру, это таргет
  df['median_city_total_square'] = df.groupby('city')['total_square'].transform('median')
  df['median_city_rooms'] = df.groupby('city')['rooms'].transform('median')
  df['median_city_floor'] = df.groupby('city')['floor'].transform('median')
  df['median_city_square_meter'] = df.groupby('city')['square_meter'].transform('median')

# введу некоторые отношения, которые тоже будут в функции. Поскольку рыночных данных очень мало, я их придумаю
def get_ratios(df: pd.DataFrame):
  # отношение площади квартиры к его
  df['ratio_district_total_square'] = df[['total_square', 'median_distict_total_square']].apply(lambda x: x['total_square'] / x['median_distict_total_square'] if pd.notna(x['total_square']) and pd.notna(x['median_distict_total_square']) and x['median_distict_total_square'] != 0 else 0.0, axis=1)
  df['ratio_city_total_square'] = df[['total_square', 'median_city_total_square']].apply(lambda x: x['total_square'] / x['median_city_total_square'] if pd.notna(x['total_square']) and pd.notna(x['median_city_total_square']) and x['median_city_total_square'] != 0 else 0.0, axis=1)
  df['difference_ratio_total_square'] = (df['ratio_city_total_square'] - df['ratio_district_total_square']).fillna(0)

  # сравнение, насколько данная квартира дороже или дешевле медианы
  # df['ratio_price_district'] = df[['price', 'median_district_price']].apply(lambda x: x['price'] / x['median_district_price'] if pd.notna(x['price']) and pd.notna(x['median_district_price']) and x['median_district_price'] != 0 else 0.0, axis=1)
  # df['ratio_price_city'] = df[['price', 'median_city_price']].apply(lambda x: x['price'] / x['median_city_price'] if pd.notna(x['price']) and pd.notna(x['median_city_price']) and x['median_city_price'] != 0 else 0.0, axis=1)
  # df['difference_ratio_price'] = (df['ratio_price_city'] - df['ratio_price_district']).fillna(0)

  # сравнение по количеству комнат
  df['ratio_rooms_district'] = df[['rooms', 'median_distict_rooms']].apply(lambda x: x['rooms'] / x['median_distict_rooms'] if pd.notna(x['rooms']) and pd.notna(x['median_distict_rooms']) and x['median_distict_rooms'] != 0 else 0.0, axis=1)
  df['ratio_rooms_city'] = df[['rooms', 'median_city_rooms']].apply(lambda x: x['rooms'] / x['median_city_rooms'] if pd.notna(x['rooms']) and pd.notna(x['median_city_rooms']) and x['median_city_rooms'] != 0 else 0.0, axis=1)
  df['difference_ratio_rooms'] = (df['ratio_rooms_city'] - df['ratio_rooms_district']).fillna(0)

# добавлю фичи для трейна и теста отдельно, потому что медианы по показателям могут быть разные
get_medians_district(X_train)
get_medians_city(X_train)
get_ratios(X_train)

get_medians_district(X_test)
get_medians_city(X_test)
get_ratios(X_test)

# важно, чтобы все показатели были числовыми
X_train.select_dtypes(include=['object']).columns

# эти колонки могут больше не пригодиться
X_train.drop(columns=['district', 'city'], axis=1, inplace=True)
X_test.drop(columns=['district', 'city'], axis=1, inplace=True)

# некоторые отношения не посчитались, по ним выпали пустые значения. Заполню их 0
missed = X_train.isna().sum()
missed[missed > 0]

# также иногда попадаются очень большие значения
X_train.replace([np.inf, -np.inf], np.nan, inplace=True)
X_train.fillna(0, inplace=True)

X_test.replace([np.inf, -np.inf], np.nan, inplace=True)
X_test.fillna(0, inplace=True)

"""# Selection
Отбор фичей
"""

# сначала я хочу посмотреть на влиятельные фичи с одном дереве, не глубоком, чтобы я мог его визуализировать
regressor = DecisionTreeRegressor(max_depth=3, random_state=42)

# Обучение
regressor.fit(X_train, y_train)

# Получаем важность признаков
feature_importances = pd.Series(regressor.feature_importances_, index=X_train.columns)

plt.figure(figsize=(20,10))
plot_tree(regressor, feature_names=X_train.columns, filled=True, rounded=True, fontsize=10)
plt.show()

# явно есть высокий вклад переменных от площади
feature_importance = pd.DataFrame({'feature': X_train.columns, 'importance': regressor.feature_importances_})
feature_importance = feature_importance.sort_values('importance', ascending=False).reset_index(drop=True)

plt.figure(figsize=(10, 6))
plt.bar(feature_importance['feature'], feature_importance['importance'])
plt.xticks(rotation=90)
plt.title('Feature Importance')
plt.tight_layout()
plt.show()

print(feature_importance)

# фукнция для получения метрик
def get_metrics(y_true, y_pred):
    mse = mean_squared_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y_true, y_pred)
    r2 = r2_score(y_true, y_pred)
    evs = explained_variance_score(y_true, y_pred)

    print("Regression Metrics:")
    print(f"Mean Squared Error (MSE): {mse:.4f}")
    print(f"Root Mean Squared Error (RMSE): {rmse:.4f}")
    print(f"Mean Absolute Error (MAE): {mae:.4f}")
    print(f"R-squared (R2) Score: {r2:.4f}")
    print(f"Explained Variance Score: {evs:.4f}")

    return {
        mse,
        rmse,
        mae,
        r2,
        evs
    }

# посмотрю на качество этого дерева. Буду считать это беслайном
y_pred = regressor.predict(X_test)
baseline_mse, baseline_rmse, baseline_mae, baseline_r2, baseline_evs = get_metrics(y_test, y_pred)

# общее количество фичей
X_train.shape

"""## VarianceThreshold"""

# предлагаю посмотреть сначала какую дисперсию они описывают
threshold = 0.0  # Хочу посмотреть все фичи
selector = VarianceThreshold(threshold=threshold)

selector.fit(X_train)
variances = selector.variances_

variance = pd.DataFrame({'Feature': X_train.columns, 'Variance': variances})
variance_sorted = variance.sort_values(by='Variance')

print(variance_sorted[:10])

# готов удалить description_ratio_upper_lower и description_count_question
# остальные могут быть полезны, а флаги редко несут высокую дисперсию. Посмотрю, как покажут себя в других методах
X_train.drop(columns=['description_ratio_upper_lower', 'description_count_question'], inplace=True)
X_test.drop(columns=['description_ratio_upper_lower', 'description_count_question'], inplace=True)

"""## mutual_info_regression"""

mi = mutual_info_regression(X_train, y_train)
mi_df = pd.DataFrame({'Feature': X_train.columns, 'Mutual Information': mi})

mi_df_sort = mi_df.sort_values(by='Mutual Information', ascending=True)
mi_df_sort.head(10)

# сентимент я хочу оставить, потому что все такие возможно он будет полезен.
# Остальные фичи убираю, с самой низкой общей информацией
mi_for_elimination = mi_df_sort[mi_df_sort.index != 25].iloc[:7]
mi_for_elimination

X_train.drop(columns=mi_for_elimination.Feature.values.tolist(), inplace=True)
X_test.drop(columns=mi_for_elimination.Feature.values.tolist(), inplace=True)

X_train.shape

"""## Random forest feature importance"""

rf_regressor = RandomForestRegressor(n_estimators=200, random_state=42)
# чем больше деревьев, тем как правило выше точность, снижается дисперсия. Я бы поставил на тысячу или больше, но очень долго считается
rf_regressor.fit(X_train, y_train)

importances = rf_regressor.feature_importances_

importance_df = pd.DataFrame({
    'feature': X_train.columns,
    'impotance': importances
})

importance_df_sorted = importance_df.sort_values(by='impotance', ascending=False)
print(importance_df_sorted)

# уберу худшие 11 фичей
X_train.drop(columns=importance_df_sorted.iloc[-11:].feature.values.tolist(), inplace=True)
X_test.drop(columns=importance_df_sorted.iloc[-11:].feature.values.tolist(), inplace=True)

"""# Оптимальное количество признаков"""

all_features = X_train.columns.tolist()

def evaluate_model(n_features, X, y):
    # использую модель LGBMRegressor, она быстрее работает
    model = lgb.LGBMRegressor(
        n_estimators=30, # небольшое количество, чтобы не было оверфиттинга и долго не считалось
        learning_rate=0.1,
        objective='regression'
    )

    selected_features = all_features[:n_features]
    X_subset = X[selected_features]

    scores = cross_val_score(model, X_subset, y, cv=5, scoring='neg_root_mean_squared_error')

    # Пусть метрикой будет RMSE
    return -scores.mean()

# Определяем диапазон количества признаков для проверки
max_features = len(all_features)
step = 3
rmse_score_crossval = []

# создам валидационный сет и трейнового
X_train_tmp, X_val, y_train_tmp, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)

# Оценка для различного числа признаков
for n_features in range(step, max_features + 1, step):
    score = evaluate_model(n_features, X_val, y_val) #
    rmse_score_crossval.append((n_features, score))
    print(f"Features: {n_features}, RMSE: {score:.4f}")

# значит достаточно 18 фичей
rmse_score_crossval

# удалю лишние признаки через известный лес. Оставлю 18 лучших признаков
X_train = X_train[importance_df_sorted[:18].feature.values.tolist()]
X_test = X_test[importance_df_sorted[:18].feature.values.tolist()]

X_train.shape

importance_df_sorted[:18]

"""#Model
Выбор модели
"""

ENR = ElasticNet().fit(Train_X_std,Train_Y)
pred1 = ENR.predict(Train_X_std)
pred2 = ENR.predict(Test_X_std)





# случайный лес
rforest = RandomForestRegressor(n_estimators=100,    # Количество деревьев в лесу
                                 max_depth=6,     # Максимальная глубина деревьев
                                 random_state=42)   # Случайное состояние для воспроизводимости
rforest.fit(X_train, y_train)
y_pred = rforest.predict(X_test)

# Оценка модели
rforest_mse_test, rforest_rmse_test, rforest_mae_test, rforest_r2_test, rforest_evs_test = get_metrics(y_test, y_pred)

y_pred_train = rforest.predict(X_train)
rforest_mse_train, rforest_rmse_train, rforest_mae_train, rforest_r2_train, rforest_evs_train = get_metrics(y_train, y_pred_train)

# разница между трейном и тестом
print(rforest_mse_train - rforest_mse_test, rforest_rmse_train - rforest_rmse_test, rforest_mae_train - rforest_mae_test, rforest_r2_train - rforest_r2_test, rforest_evs_train - rforest_evs_test)

# catboost
catboost = CatBoostRegressor(
    iterations=1000,
    learning_rate=0.1,
    depth=6,
    loss_function='RMSE',
    verbose=200
)

catboost.fit(X_train, y_train)
y_pred = catboost.predict(X_test)

# Оценка модели
catboost_mse_test, catboost_rmse_test, catboost_mae_test, catboost_r2_test, catboost_evs_test = get_metrics(y_test, y_pred)

y_pred_train = catboost.predict(X_train)
catboost_mse_train, catboost_rmse_train, catboost_mae_train, catboost_r2_train, catboost_evs_train = get_metrics(y_train, y_pred_train)

# разница между трейном и тестом
print(catboost_mse_train - catboost_mse_test, catboost_rmse_train - catboost_rmse_test, catboost_mae_train - catboost_mae_test, catboost_r2_train - catboost_r2_test, catboost_evs_train - catboost_evs_test)

# lgbm
lgbm = lgb.LGBMRegressor(
        n_estimators=1000,
        learning_rate=0.1,
        objective='regression'
    )

lgbm.fit(X_train, y_train)
y_pred = lgbm.predict(X_test)

# Оценка модели
lgbm_mse_test, lgbm_rmse_test, lgbm_mae_test, lgbm_r2_test, lgbm_evs_test = get_metrics(y_test, y_pred)

y_pred_train = lgbm.predict(X_train)
lgbm_mse_train, lgbm_rmse_train, lgbm_mae_train, lgbm_r2_train = get_metrics(y_train, y_pred_train)

# разница между трейном и тестом
print(lgbm_mse_train - lgbm_mse_test, lgbm_rmse_train - lgbm_rmse_test, lgbm_mae_train - lgbm_mae_test, lgbm_r2_train - lgbm_r2_test)

"""#hyperparams
Гиперпараметры. Важная часть построенияю модели. Долго считается

[Выбор модели](#Model) \

#quality
Оценка качества
"""

metrics_lgbm = [lgbm_mse_test, lgbm_rmse_test, lgbm_mae_test, lgbm_r2_test]
metrics_catboost = [catboost_mse_test, catboost_rmse_test, catboost_mae_test, catboost_r2_test]
metrics_rforest = [rforest_mse_test, rforest_rmse_test, rforest_mae_test, rforest_r2_test]

metrics = ['MSE Test', 'RMSE Test', 'MAE Test', 'R2 Test']
models = ['LGBM', 'CatBoost', 'Random Forest']

data = {
    'Model': [],
    'Metric': [],
    'Value': []
}

for i, model in enumerate(models):
    for j, metric in enumerate(metrics):
        data['Model'].append(model)
        data['Metric'].append(metric)
        if model == 'LGBM':
            data['Value'].append(metrics_lgbm[j])
        elif model == 'CatBoost':
            data['Value'].append(metrics_catboost[j])
        elif model == 'Random Forest':
            data['Value'].append(metrics_rforest[j])

df = pd.DataFrame(data)

plt.figure(figsize=(12, 8))

sns.barplot(x='Metric', y='Value', hue='Model', data=df, palette='viridis')
plt.show()

"""Модели показывают высокое качество, мне больше нравится модель LGBM

"""

